{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os, sys, re, gc, pickle, operator, shutil, copy\n",
    "import time, datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from math import floor, ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, Sampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainedModel\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/google-quest-challenge/\"\n",
    "# BERT_MODEL_PATH = '/kaggle/input/bert-pretrained-models/cased_L-12_H-768_A-12/cased_L-12_H-768_A-12/'\n",
    "BERT_MODEL_PATH = '/kaggle/input/bert-pretrained-models/uncased_L-12_H-768_A-12/uncased_L-12_H-768_A-12/'\n",
    "# BERT_MODEL_PATH = '/kaggle/input/bert-pretrained-models/uncased_L-24_H-1024_A-16/uncased_L-24_H-1024_A-16/'\n",
    "\n",
    "MODEL_PATH1 = '../input/gq-bert-cls-1/'\n",
    "MODEL_PATH2 = '../input/gq-bert-pool-aug/'\n",
    "# MODEL_PATH2 = '../input/gq-2bert-pool-1/'\n",
    "# MODEL_PATH2_ap = '../input/gq-2bert-pool-2/'\n",
    "# MODEL_PATH2 = '../input/gq-bert-pool-exp/'\n",
    "# MODEL_PATH2 = '../input/gq-bert-pool/'\n",
    "# MODEL_PATH2 = '../input/gq-bert-pool-cased/'\n",
    "# MODEL_PATH2 = '../input/gq-bert-large-pool/'\n",
    "MODEL_PATH3 = '../input/gq-bert-2pool-exp/'\n",
    "# MODEL_PATH3_1 = '../input/gq-bert-2pool-exp3/'\n",
    "# MODEL_PATH3_2 = '../input/gq-bert-2pool-exp4/'\n",
    "# MODEL_PATH3_3 = '../input/gq-bert-2pool-exp5/'\n",
    "MODEL_PATH3_1 = '../input/gq-bert-2pool/'\n",
    "MODEL_PATH3_2 = '../input/gq-bert-2pool-exp8/'\n",
    "MODEL_PATH3_3 = '../input/gq-bert-2pool-alltrunc/'\n",
    "# MODEL_PATH3 = '../input/gq-bert-2pool-cased/'\n",
    "MODEL_PATH4 = '../input/gq-bert-cls-pool/'\n",
    "MODEL_PATH5 = '../input/gq-bert-4pool-exp/'\n",
    "# MODEL_PATH5 = '../input/gq-bert-last4/'\n",
    "MODEL_PATH6 = '../input/gq-bert-2catpool/'\n",
    "MODEL_PATH7 = '../input/gq-bert-3catpool/'\n",
    "MODEL_PATH8 = '../input/gq-bert-3pool/'\n",
    "\n",
    "\n",
    "SEED = 2019\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for randomness in pytorch\n",
    "def seed_torch(seed=SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of data preprocessing and pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thanks to https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic\n",
    "# def trim_and_tokenize(title, question, answer, max_sequence_length, tokenizer,\n",
    "#                       t_max_len=18, q_max_len=245, a_max_len=244):\n",
    "\n",
    "#     tq_sep = tokenizer.tokenize(\"Details:\")\n",
    "#     t = tokenizer.tokenize(title)\n",
    "#     q = tokenizer.tokenize(question)\n",
    "#     a = tokenizer.tokenize(answer)\n",
    "    \n",
    "#     t_len = len(t)\n",
    "#     q_len = len(q)\n",
    "#     a_len = len(a)\n",
    "\n",
    "#     if (t_len+q_len+a_len+5) > max_sequence_length:\n",
    "        \n",
    "#         if t_max_len > t_len:\n",
    "#             t_new_len = t_len\n",
    "#             a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "#             q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "#         else:\n",
    "#             t_new_len = t_max_len\n",
    "      \n",
    "#         if a_max_len > a_len:\n",
    "#             a_new_len = a_len \n",
    "#             q_new_len = q_max_len + (a_max_len - a_len)\n",
    "#         elif q_max_len > q_len:\n",
    "#             a_new_len = a_max_len + (q_max_len - q_len)\n",
    "#             q_new_len = q_len\n",
    "#         else:\n",
    "#             a_new_len = a_max_len\n",
    "#             q_new_len = q_max_len\n",
    "            \n",
    "#         if t_new_len+a_new_len+q_new_len+5 != max_sequence_length:\n",
    "#             raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "#                              % (max_sequence_length, (t_new_len+a_new_len+q_new_len+5)))\n",
    "        \n",
    "# #         t = t[-t_new_len:]\n",
    "# #         q = q[-q_new_len:]\n",
    "# #         a = a[-a_new_len:]\n",
    "#         t = t[:t_new_len]\n",
    "#         q = q[:q_new_len]\n",
    "#         a = a[:a_new_len]\n",
    "    \n",
    "#     return t, tq_sep, q, a\n",
    "\n",
    "\n",
    "# def trim_and_tokenize_2(title, question, answer, tokenizer,\n",
    "#                       t_max_len=20, q_max_len=300, a_max_len=300):\n",
    "\n",
    "#     t = tokenizer.tokenize(title)\n",
    "#     q = tokenizer.tokenize(question)\n",
    "#     a = tokenizer.tokenize(answer)\n",
    "    \n",
    "#     t = t[:t_max_len]\n",
    "#     q = q[:q_max_len]\n",
    "#     a = a[:a_max_len]\n",
    "    \n",
    "#     return t, q, a\n",
    "\n",
    "\n",
    "# # Tokenizing the lines to BERT token\n",
    "# def convert_lines(df, columns, max_sequence_length, tokenizer):\n",
    "#     all_tokens = []\n",
    "#     segment_ids = []   # representing segmentation of sentence A and B\n",
    "    \n",
    "#     for _, instance in df[columns].iterrows():\n",
    "#         t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "#         t, tq_sep, q, a = trim_and_tokenize(t, q, a, max_sequence_length, tokenizer)\n",
    "#         tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "#         all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "#         segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "    \n",
    "#     return np.array(all_tokens), np.array(segment_ids)\n",
    "\n",
    "\n",
    "# def convert_lines_2(df, columns, tokenizer):\n",
    "#     q_all_tokens = []\n",
    "#     a_all_tokens = []\n",
    "#     q_segment_ids = []   # representing segmentation of sentence A and B\n",
    "    \n",
    "#     for _, instance in df[columns].iterrows():\n",
    "#         t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "#         t, q, a = trim_and_tokenize_2(t, q, a, tokenizer)\n",
    "#         q_tokens = [\"[CLS]\"] + t + [\"[SEP]\"] + q + [\"[SEP]\"]\n",
    "#         a_tokens = [\"[CLS]\"] + a + [\"[SEP]\"]\n",
    "#         q_all_tokens.append(tokenizer.convert_tokens_to_ids(q_tokens))\n",
    "#         a_all_tokens.append(tokenizer.convert_tokens_to_ids(a_tokens))\n",
    "#         q_segment_ids.append([0]*(len(t)+2) + [1]*(len(q)+1))\n",
    "    \n",
    "#     return np.array(q_all_tokens), np.array(a_all_tokens), np.array(q_segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to https://www.kaggle.com/akensert/bert-base-tf2-0-minimalistic\n",
    "def trim_and_tokenize(title, question, answer, max_sequence_length, tokenizer,\n",
    "                      trunc_mode='head', t_max_len=18, q_max_len=245, a_max_len=244):\n",
    "    \n",
    "    assert trunc_mode in {\"head\", \"tail\", \"mix\"}\n",
    "    need_trunc = False\n",
    "\n",
    "    tq_sep = tokenizer.tokenize(\"Details:\")\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+5) > max_sequence_length:\n",
    "        need_trunc = True\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+5 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+5)))\n",
    "        \n",
    "        if trunc_mode == \"head\":\n",
    "            t = t[:t_new_len]\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "        if trunc_mode == \"tail\":\n",
    "            t = t[-t_new_len:]\n",
    "            q = q[-q_new_len:]\n",
    "            a = a[-a_new_len:]\n",
    "        if trunc_mode == \"mix\":\n",
    "            def trunc_seq(seq, seq_max_len, trunc_ratio=0.6):\n",
    "                maj_len = int(seq_max_len * trunc_ratio)\n",
    "                return seq[:maj_len] + seq[-(seq_max_len-maj_len):]\n",
    "            t = trunc_seq(t, t_new_len)\n",
    "            q = trunc_seq(q, q_new_len)\n",
    "            a = trunc_seq(a, a_new_len)\n",
    "    \n",
    "    return t, tq_sep, q, a, need_trunc\n",
    "\n",
    "\n",
    "# Tokenizing the lines to BERT token\n",
    "def convert_lines(df, columns, max_sequence_length, tokenizer, trunc_mode='head', misc_trunc=False, target=None):\n",
    "    all_tokens = []\n",
    "    segment_ids = []   # representing segmentation of sentence A and B\n",
    "    if target is not None:\n",
    "        labels = []\n",
    "    \n",
    "    for ind, (_, instance) in enumerate(df[columns].iterrows()):\n",
    "        \n",
    "        title, question, answer = instance.question_title, instance.question_body, instance.answer\n",
    "        t, tq_sep, q, a, need_trunc = trim_and_tokenize(title, question, answer,\n",
    "                                                        max_sequence_length, tokenizer, trunc_mode=trunc_mode)\n",
    "        tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "        all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "        segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "        if target is not None:\n",
    "            labels.append(target[ind])\n",
    "        \n",
    "        if need_trunc and misc_trunc:\n",
    "            t, tq_sep, q, a, _ = trim_and_tokenize(title, question, answer,\n",
    "                                                   max_sequence_length, tokenizer, trunc_mode='tail')\n",
    "            tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "            all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "\n",
    "            t, tq_sep, q, a, _ = trim_and_tokenize(title, question, answer,\n",
    "                                                   max_sequence_length, tokenizer, trunc_mode='mix')\n",
    "            tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "            all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "\n",
    "            if target is not None:\n",
    "                labels.extend([target[ind], target[ind]])\n",
    "    \n",
    "    if target is not None:\n",
    "        return np.array(all_tokens), np.array(segment_ids), np.array(labels)\n",
    "    return np.array(all_tokens), np.array(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "\n",
    "class QuestQAs(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_comments, segment_ids, targets=None, split=None, maxlen=256):\n",
    "        self.comments = tokenized_comments\n",
    "        self.segment_ids = segment_ids\n",
    "        self.targets = targets\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'valid', 'test'}\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.comments[index]\n",
    "        segment_id = self.segment_ids[index]\n",
    "        if self.targets is not None:\n",
    "            target = self.targets[index]\n",
    "            return comment, segment_id, torch.FloatTensor(target)\n",
    "        else:\n",
    "            return comment, segment_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def get_lens(self):\n",
    "        lengths = np.fromiter(\n",
    "            ((min(self.maxlen, len(seq))) for seq in self.comments),\n",
    "            dtype=np.int32)\n",
    "        return lengths\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for sequence bucketing\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of comments, and targets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            comments, segment_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            comments, segment_ids = zip(*batch)\n",
    "\n",
    "        lengths = [len(c) for c in comments]\n",
    "        maxlen = max(lengths)\n",
    "        padded_comments, padded_seg_ids = [], []\n",
    "        for i, (c, s) in enumerate(zip(comments, segment_ids)):\n",
    "            padded_comments.append(c+[0]*(maxlen - lengths[i]))\n",
    "            padded_seg_ids.append(s +[0]*(maxlen - lengths[i]))\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            return torch.LongTensor(padded_comments), torch.LongTensor(padded_seg_ids), torch.stack(targets)\n",
    "        else:\n",
    "            return torch.LongTensor(padded_comments), torch.LongTensor(padded_seg_ids)\n",
    "\n",
    "\n",
    "class QuestQAs2(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_q_comments, tokenized_a_comments, q_segment_ids, targets=None, split=None, maxlen=256):\n",
    "        self.q_comments = tokenized_q_comments\n",
    "        self.a_comments = tokenized_a_comments\n",
    "        self.q_segment_ids = q_segment_ids\n",
    "        self.targets = targets\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'valid', 'test'}\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        q_comment = self.q_comments[index]\n",
    "        a_comment = self.a_comments[index]\n",
    "        q_segment_id = self.q_segment_ids[index]\n",
    "        if self.targets is not None:\n",
    "            target = self.targets[index]\n",
    "            return q_comment, a_comment, q_segment_id, torch.FloatTensor(target)\n",
    "        else:\n",
    "            return q_comment, a_comment, q_segment_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.q_comments)\n",
    "\n",
    "    def get_lens(self):\n",
    "        lengths = np.fromiter(\n",
    "            ((min(self.maxlen, len(seq))) for seq in self.q_comments),\n",
    "            dtype=np.int32)\n",
    "        return lengths\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for sequence bucketing\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of comments, and targets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            q_comments, a_comments, q_segment_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            q_comments, a_comments, q_segment_ids = zip(*batch)\n",
    "\n",
    "        q_lengths = [len(c) for c in q_comments]\n",
    "        a_lengths = [len(c) for c in a_comments]\n",
    "        q_maxlen = max(q_lengths)\n",
    "        a_maxlen = max(a_lengths)\n",
    "        padded_q_comments, padded_a_comments, padded_q_seg_ids = [], [], []\n",
    "        for i, (q, a, s) in enumerate(zip(q_comments, a_comments, q_segment_ids)):\n",
    "            padded_q_comments.append(q+[0]*(q_maxlen - q_lengths[i]))\n",
    "            padded_a_comments.append(a+[0]*(a_maxlen - a_lengths[i]))\n",
    "            padded_q_seg_ids.append(s +[0]*(q_maxlen - q_lengths[i]))\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            return torch.LongTensor(padded_q_comments), torch.LongTensor(padded_a_comments), \\\n",
    "                    torch.LongTensor(padded_q_seg_ids), torch.stack(targets)\n",
    "        else:\n",
    "            return torch.LongTensor(padded_q_comments), torch.LongTensor(padded_a_comments), torch.LongTensor(padded_q_seg_ids)\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_lens, bucket_size=None, batch_size=1024, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.sort_lens = sort_lens\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_lens)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle_data\n",
    "        \n",
    "        self.weights = None\n",
    "        if not shuffle_data:\n",
    "            self.index = self.prepare_buckets()\n",
    "        else:\n",
    "            self.index = None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        assert weights >= 0\n",
    "        self.weights = weights / np.sum(weights)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_lens)\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_lens)\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = self.prepare_buckets(indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def prepare_buckets(self, indices=None):\n",
    "        lengths = - self.sort_lens  # negative sign for descending sort later\n",
    "        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lengths)\n",
    "\n",
    "        if indices is None:\n",
    "            if self.shuffle:\n",
    "                indices = shuffle(np.arange(len(lengths), dtype=np.int32))\n",
    "                lengths = lengths[indices]\n",
    "            else:\n",
    "                indices = np.arange(len(lengths), dtype=np.int32)\n",
    "\n",
    "        def divide_chunks(l, n):\n",
    "            \"\"\"bucket iterator\n",
    "            l: data to be divided\n",
    "            n: number of chunks\n",
    "            \"\"\"\n",
    "            if n == len(l):\n",
    "                yield np.arange(len(l), dtype=np.int32), l\n",
    "            else:\n",
    "                # looping till length of l\n",
    "                for i in range(0, len(l), n):\n",
    "                    data = l[i:i + n]\n",
    "                    yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "\n",
    "        new_indices = []\n",
    "        extra_batch_idx = None\n",
    "        for chunk_indices, chunk in divide_chunks(lengths, self.bucket_size):\n",
    "            # sort indices in bucket by descending order of lengths\n",
    "            indices_sorted = chunk_indices[np.argsort(chunk)]\n",
    "\n",
    "            batch_idxes = []\n",
    "            for _, batch_idx in divide_chunks(indices_sorted, self.batch_size):\n",
    "                if len(batch_idx) == self.batch_size:\n",
    "                    batch_idxes.append(batch_idx.tolist())\n",
    "                else:\n",
    "                    assert extra_batch_idx is None\n",
    "                    assert batch_idx is not None\n",
    "                    extra_batch_idx = batch_idx.tolist()\n",
    "\n",
    "            # shuffling batches within buckets\n",
    "            if self.shuffle:\n",
    "                batch_idxes = shuffle(batch_idxes)\n",
    "            for batch_idx in batch_idxes:\n",
    "                new_indices.extend(batch_idx)\n",
    "\n",
    "        if extra_batch_idx is not None:\n",
    "            new_indices.extend(extra_batch_idx)\n",
    "\n",
    "        if not self.shuffle:\n",
    "            self.original_indices = np.argsort(indices_sorted).tolist()\n",
    "        return indices[new_indices]\n",
    "\n",
    "\n",
    "def prepare_loader(x, seg_ids, y=None, batch_size=None, split=None):\n",
    "    assert split in {'train', 'valid', 'test'}\n",
    "    dataset = QuestQAs(x, seg_ids, y, split, MAX_SEQUENCE_LENGTH)\n",
    "    if split == 'train':\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                bucket_size=batch_size*20, batch_size=batch_size)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                batch_size=batch_size, shuffle_data=False)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn), sampler.original_indices\n",
    "\n",
    "\n",
    "def prepare_loader_2(q, a, q_seg_ids, y=None, batch_size=None, split=None):\n",
    "    assert split in {'train', 'valid', 'test'}\n",
    "    dataset = QuestQAs2(q, a, q_seg_ids, y, split, MAX_SEQUENCE_LENGTH)\n",
    "    if split == 'train':\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                bucket_size=batch_size*20, batch_size=batch_size)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                batch_size=batch_size, shuffle_data=False)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn), sampler.original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertQAClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertQAClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert_q = BertModel(config)\n",
    "        self.bert_a = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(config.hidden_size*2, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_q_ids, input_a_ids, q_token_type_ids=None, a_token_type_ids=None, \n",
    "                q_attention_mask=None, a_attention_mask=None):\n",
    "        seq_q_output, _ = self.bert_q(input_q_ids, q_token_type_ids, q_attention_mask, output_all_encoded_layers=False)\n",
    "        seq_a_output, _ = self.bert_a(input_a_ids, a_token_type_ids, a_attention_mask, output_all_encoded_layers=False)\n",
    "        sequence_output = torch.cat((seq_q_output, seq_a_output), 1)\n",
    "        avg_pool = torch.mean(sequence_output, 1)\n",
    "        max_pool, _ = torch.max(sequence_output, 1)\n",
    "        pooled_output = torch.cat((avg_pool, max_pool), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForCLSClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertForCLSClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "\n",
    "class BertForPoolClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertForPoolClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*2, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        avg_pool = torch.mean(sequence_output, 1)\n",
    "        max_pool, _ = torch.max(sequence_output, 1)\n",
    "        pooled_output = torch.cat((avg_pool, max_pool), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "\n",
    "class BertLastTwoClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertLastTwoClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*4, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "\n",
    "class BertCLSLastTwoClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertCLSLastTwoClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*5, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, pooled_cls = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2, pooled_cls), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "\n",
    "class BertLastFourClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertLastFourClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*8, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, pooled_cls = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        seq_op3 = encoded_layers[-3]\n",
    "        seq_op4 = encoded_layers[-4]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        avg_pool3 = torch.mean(seq_op3, 1)\n",
    "        max_pool3, _ = torch.max(seq_op3, 1)\n",
    "        avg_pool4 = torch.mean(seq_op4, 1)\n",
    "        max_pool4, _ = torch.max(seq_op4, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2,\n",
    "                                   avg_pool3, max_pool3, avg_pool4, max_pool4), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "    \n",
    "    \n",
    "class BertCatLastTwoClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertCatLastTwoClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*4, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]   # (N, T, D)\n",
    "        seq_op2 = encoded_layers[-2]   # (N, T, D)\n",
    "        seq_cat = torch.cat((seq_op1, seq_op2), -1)   # (N, T, 2D)\n",
    "        avg_pool = torch.mean(seq_cat, 1)   # (N, 2D)\n",
    "        max_pool, _ = torch.max(seq_cat, 1)   # (N, 2D)\n",
    "        pooled_output = torch.cat((avg_pool, max_pool), 1)   # (N, 4D)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "    \n",
    "\n",
    "class BertLastThreeClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertLastThreeClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.classifier = nn.Linear(config.hidden_size*6, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        seq_op3 = encoded_layers[-3]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        avg_pool3 = torch.mean(seq_op3, 1)\n",
    "        max_pool3, _ = torch.max(seq_op3, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2, avg_pool3, max_pool3), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))\n",
    "\n",
    "\n",
    "class BertCatLastThreeClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertCatLastThreeClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*6, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]   # (N, T, D)\n",
    "        seq_op2 = encoded_layers[-2]   # (N, T, D)\n",
    "        seq_op3 = encoded_layers[-3]   # (N, T, D)\n",
    "        seq_cat = torch.cat((seq_op1, seq_op2, seq_op3), -1)   # (N, T, 3D)\n",
    "        avg_pool = torch.mean(seq_cat, 1)   # (N, 3D)\n",
    "        max_pool, _ = torch.max(seq_cat, 1)   # (N, 3D)\n",
    "        pooled_output = torch.cat((avg_pool, max_pool), 1)   # (N, 6D)\n",
    "        return self.classifier(self.dropout(pooled_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertGroup1(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BertGroup1, self).__init__()\n",
    "        self.embeddings = bert.embeddings\n",
    "        self.bert_encoder = copy.deepcopy(bert).encoder.layer[:8]\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "        \n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        hidden_states = self.embeddings(input_ids, token_type_ids)\n",
    "        for layer_module in self.bert_encoder:\n",
    "            hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "            \n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertGroup2(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BertGroup2, self).__init__()\n",
    "        self.bert_encoder = copy.deepcopy(bert).encoder.layer[:8]\n",
    "        \n",
    "    def forward(self, hidden_states, input_ids, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        \n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        \n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.bert_encoder:\n",
    "            hidden_states = layer_module(hidden_states, extended_attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "            \n",
    "        return all_encoder_layers\n",
    "    \n",
    "\n",
    "class BertGroupsClassification(nn.Module):\n",
    "    def __init__(self, bert, config, num_labels):\n",
    "        super(BertGroupsClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert_group_1 = BertGroup1(bert)\n",
    "        self.bert_group_2 = BertGroup2(bert)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size*4, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        hidden_states = self.bert_group_1(input_ids, token_type_ids, attention_mask)\n",
    "        encoded_layers = self.bert_group_2(hidden_states, input_ids, attention_mask)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2), 1)\n",
    "        return self.classifier(self.dropout(pooled_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for validation and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rho(labels, preds):\n",
    "    rhos = []\n",
    "    for col_label, col_pred in zip(labels.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_label, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "\n",
    "def validate(val_loader, model, val_df, val_original_indices):\n",
    "    model.eval()\n",
    "    targets, scores = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device=device, dtype=torch.long), y.to(device=device, dtype=torch.float)\n",
    "            score = model(x, attention_mask=(x>0))\n",
    "            targets.append(y.cpu().numpy())\n",
    "            scores.append(torch.sigmoid(score).cpu().numpy())\n",
    "\n",
    "    targets = np.concatenate(targets)\n",
    "    scores = np.concatenate(scores)\n",
    "    val_rho = compute_rho(targets, scores)\n",
    "    print('{\"metric\": \"Val. Rho\", \"value\": %.4f}' % (val_arho, ))\n",
    "    \n",
    "#     val_scores = scores[val_original_indices]\n",
    "#     return val_scores\n",
    "\n",
    "\n",
    "def eval_model(model, test_loader):\n",
    "    model.eval()\n",
    "    test_scores = []\n",
    "    with torch.no_grad():\n",
    "        for x, seg_ids in test_loader:\n",
    "            x = x.to(device=device, dtype=torch.long)\n",
    "            seg_ids = seg_ids.to(device=device, dtype=torch.long)\n",
    "            score = torch.sigmoid(model(x, token_type_ids=seg_ids, attention_mask=(x>0)))\n",
    "            test_scores.append(score.cpu().numpy())\n",
    "    return np.concatenate(test_scores)\n",
    "\n",
    "\n",
    "def eval_model_2(model, test_loader):\n",
    "    model.eval()\n",
    "    test_scores = []\n",
    "    with torch.no_grad():\n",
    "        for q, a, seg_ids in test_loader:\n",
    "            q, a = q.to(device=device, dtype=torch.long), a.to(device=device, dtype=torch.long)\n",
    "            seg_ids = seg_ids.to(device=device, dtype=torch.long)\n",
    "            score = torch.sigmoid(model(q, a, q_token_type_ids=seg_ids, \n",
    "                                  q_attention_mask=(q>0), a_attention_mask=(a>0)))\n",
    "            test_scores.append(score.cpu().numpy())\n",
    "    return np.concatenate(test_scores)\n",
    "\n",
    "\n",
    "class NNAverage(object):\n",
    "    def __init__(self, model, mu=0.5):\n",
    "        self.mu = mu\n",
    "        self.weight_copy = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.weight_copy[name] = 0\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.weight_copy[name] += self.mu * param.data\n",
    "\n",
    "    def set_weights(self, avg_model):\n",
    "        for name, param in avg_model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data = self.weight_copy[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load/preprocess the data, and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_df):\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "#     cv_indices = [(tr_idx, val_idx) for tr_idx, val_idx in kf.split(train_df.question_body, groups=train_df.question_body)]\n",
    "    cv_indices = [(tr_idx, val_idx) for tr_idx, val_idx in kf.split(train_df)]\n",
    "    return cv_indices\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    test_df = pd.read_csv(DATA_DIR+'test.csv')\n",
    "    sub_df = pd.read_csv(DATA_DIR+'sample_submission.csv')\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    output_cols = list(train_df.columns[11:])\n",
    "    input_cols = list(train_df.columns[[1,2,5]])\n",
    "    \n",
    "    test_seq, test_seg_ids = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "    return train_df, test_df, sub_df, input_cols, output_cols\n",
    "\n",
    "\n",
    "def load_and_preproc():\n",
    "    test_df = pd.read_csv(DATA_DIR+'test.csv')\n",
    "    sub_df = pd.read_csv(DATA_DIR+'sample_submission.csv')\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    input_cols = list(train_df.columns[[1,2,5]])\n",
    "    \n",
    "    test_seq, test_seg_ids = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "    return test_seq, test_seg_ids, test_df, sub_df\n",
    "\n",
    "\n",
    "def load_and_preproc_2():\n",
    "    test_df = pd.read_csv(DATA_DIR+'test.csv')\n",
    "    sub_df = pd.read_csv(DATA_DIR+'sample_submission.csv')\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    input_cols = list(train_df.columns[[1,2,5]])\n",
    "    \n",
    "    test_q_seq, test_a_seq, test_q_seg_ids = convert_lines_2(test_df, input_cols, tokenizer)\n",
    "\n",
    "    return test_q_seq, test_a_seq, test_q_seg_ids, test_df, sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.82 s, sys: 41.4 ms, total: 5.86 s\n",
      "Wall time: 5.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=True)\n",
    "# tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH)\n",
    "\n",
    "# t0 = time.time()\n",
    "# print('Loading and tokenizing...')\n",
    "# x_test, seg_test, test_df, sub_df = load_and_preproc()\n",
    "# x_q_test, x_a_test, seg_q_test, test_df, sub_df = load_and_preproc_2()\n",
    "train_df, test_df, sub_df, input_cols, output_cols = load_data()\n",
    "# print('tokenizing complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n",
      "tokenizing complete in 11 seconds.\n",
      "CPU times: user 11.3 s, sys: 72 µs, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = time.time()\n",
    "print('Tokenizing...')\n",
    "\n",
    "x_test, seg_test = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "x_test_2, seg_test_2 = convert_lines(test_df, input_cols,\n",
    "                                     MAX_SEQUENCE_LENGTH, tokenizer,\n",
    "                                     trunc_mode='tail')\n",
    "\n",
    "print('tokenizing complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference preparation\n",
    "\n",
    "test_loader, test_original_indices = prepare_loader(x_test, seg_test, batch_size=batch_size, split='test')\n",
    "test_loader_2, test_original_indices_2 = prepare_loader(x_test_2, seg_test_2, batch_size=batch_size, split='test')\n",
    "# test_loader, test_original_indices = prepare_loader_2(x_q_test, x_a_test, seg_q_test, batch_size=batch_size, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0M\n",
      "0.0M\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT configuration file for model loading\n",
    "bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n",
    "\n",
    "# model setup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# # model = BertForCLSClassification(bert_config, num_labels=30)\n",
    "# model = BertForPoolClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# models = torch.load(MODEL_PATH+'quest_bert_models.pt')['model']\n",
    "# model.load_state_dict(models[list(models.keys())[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# # val_preds = validate(val_loader, model, val_df, val_original_indices)\n",
    "\n",
    "# test_preds = eval_model(model, test_loader)[test_original_indices]\n",
    "# # test_preds = []\n",
    "# # ckpt_weights = [1.1**e for e in range(len(models))]\n",
    "# # for state in models.values():\n",
    "# #     model.load_state_dict(state)\n",
    "# #     test_preds.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds = np.average(test_preds, weights=ckpt_weights, axis=0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # weight avg of models from diff folds\n",
    "# print(\"Building model...\")\n",
    "# model = BertForCLSClassification(bert_config, num_labels=30)\n",
    "# avgd = NNAverage(model, 1./5)\n",
    "\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH1+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     avgd.update(model)\n",
    "\n",
    "# print('Averaging and inference...')\n",
    "# avgd.set_weights(model)\n",
    "# model = model.to(device)\n",
    "# test_preds = eval_model(model, test_loader)[test_original_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertForCLSClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_1 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH1+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_1.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# test_preds_1 = np.mean(test_preds_1, 0)\n",
    "# # test_preds = np.mean(test_preds_1, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertQAClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_2 = []\n",
    "# for i in range(5):\n",
    "#     if i < 3:\n",
    "#         models = torch.load(MODEL_PATH2+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     else:\n",
    "#         models = torch.load(MODEL_PATH2_ap+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_2.append(eval_model_2(model, test_loader)[test_original_indices])\n",
    "# # test_preds_2 = np.mean(test_preds_2, 0)\n",
    "# test_preds = np.mean(test_preds_2, 0)\n",
    "\n",
    "# # models = torch.load(MODEL_PATH2+f'quest_bert_models.pt')['model']\n",
    "# # model.load_state_dict(models[list(models.keys())[-1]])\n",
    "# # test_preds = eval_model(model, test_loader)[test_original_indices]\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertForPoolClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_2 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH2+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_2.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds_2 = np.mean(test_preds_2, 0)\n",
    "# test_preds = np.mean(test_preds_2, 0)\n",
    "\n",
    "# # models = torch.load(MODEL_PATH2+f'quest_bert_models.pt')['model']\n",
    "# # model.load_state_dict(models[list(models.keys())[-1]])\n",
    "# # test_preds = eval_model(model, test_loader)[test_original_indices]\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0m 33s.\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = BertLastTwoClassification(bert_config, num_labels=30)\n",
    "model = model.to(device)\n",
    "\n",
    "# test inference\n",
    "t0 = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_preds_3 = []\n",
    "for i in range(5):\n",
    "    models = torch.load(MODEL_PATH3+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "    model.load_state_dict(models[list(models.keys())[-1]])\n",
    "    test_preds_3.append(eval_model(model, test_loader)[test_original_indices])\n",
    "test_preds_3 = np.mean(test_preds_3, 0)\n",
    "# test_preds = np.mean(test_preds_3, 0)\n",
    "\n",
    "time_elapsed = time.time() - t0\n",
    "print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0m 32s.\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = BertLastTwoClassification(bert_config, num_labels=30)\n",
    "model = model.to(device)\n",
    "\n",
    "# test inference\n",
    "t0 = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_preds_3_1 = []\n",
    "for i in range(5):\n",
    "    models = torch.load(MODEL_PATH3_1+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "    model.load_state_dict(models[list(models.keys())[-1]])\n",
    "    test_preds_3_1.append(eval_model(model, test_loader_2)[test_original_indices_2])\n",
    "test_preds_3_1 = np.mean(test_preds_3_1, 0)\n",
    "# test_preds = np.mean(test_preds_3_1, 0)\n",
    "\n",
    "time_elapsed = time.time() - t0\n",
    "print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertLastTwoClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_3_2 = []\n",
    "# for i in range(5):\n",
    "#     if i==1:continue\n",
    "#     models = torch.load(MODEL_PATH3_2+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_3_2.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds_3_2 = np.mean(test_preds_3_2, 0)\n",
    "# test_preds = np.mean(test_preds_3_2, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertCLSLastTwoClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_4 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH4+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_4.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# test_preds_4 = np.mean(test_preds_4, 0)\n",
    "# # test_preds = np.mean(test_preds_4, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertLastFourClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_5 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH5+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_5.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds_5 = np.mean(test_preds_5, 0)\n",
    "# test_preds = np.mean(test_preds_5, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 0m 32s.\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = BertCatLastTwoClassification(bert_config, num_labels=30)\n",
    "model = model.to(device)\n",
    "\n",
    "# test inference\n",
    "t0 = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_preds_6 = []\n",
    "for i in range(5):\n",
    "    models = torch.load(MODEL_PATH6+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "    model.load_state_dict(models[list(models.keys())[-1]])\n",
    "    test_preds_6.append(eval_model(model, test_loader)[test_original_indices])\n",
    "test_preds_6 = np.mean(test_preds_6, 0)\n",
    "# test_preds = np.mean(test_preds_6, 0)\n",
    "\n",
    "time_elapsed = time.time() - t0\n",
    "print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time = 2m 53s.\n"
     ]
    }
   ],
   "source": [
    "# model setup\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Building model...\")\n",
    "model = BertLastTwoClassification(bert_config, num_labels=30)\n",
    "model = model.to(device)\n",
    "\n",
    "# test inference\n",
    "t0 = time.time()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "test_preds_3_3_head = []\n",
    "test_preds_3_3_tail = []\n",
    "test_preds_3_3_mix = []\n",
    "for i in range(5):\n",
    "    models = torch.load(MODEL_PATH3_3+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "    model.load_state_dict(models[list(models.keys())[-1]])\n",
    "    \n",
    "    x_test, seg_test = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    test_loader, test_original_indices = prepare_loader(x_test, seg_test, batch_size=batch_size, split='test')\n",
    "    test_preds_3_3_head.append(eval_model(model, test_loader)[test_original_indices])\n",
    "    \n",
    "    x_test, seg_test = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer, trunc_mode='tail')\n",
    "    test_loader, test_original_indices = prepare_loader(x_test, seg_test, batch_size=batch_size, split='test')\n",
    "    test_preds_3_3_tail.append(eval_model(model, test_loader)[test_original_indices])\n",
    "    \n",
    "    x_test, seg_test = convert_lines(test_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer, trunc_mode='mix')\n",
    "    test_loader, test_original_indices = prepare_loader(x_test, seg_test, batch_size=batch_size, split='test')\n",
    "    test_preds_3_3_mix.append(eval_model(model, test_loader)[test_original_indices])\n",
    "\n",
    "test_preds_3_3_head = np.mean(test_preds_3_3_head, 0)\n",
    "test_preds_3_3_tail = np.mean(test_preds_3_3_tail, 0)\n",
    "test_preds_3_3_mix = np.mean(test_preds_3_3_mix, 0)\n",
    "\n",
    "test_preds_3_3 = np.mean([test_preds_3_3_head, test_preds_3_3_tail, test_preds_3_3_mix], 0)\n",
    "# test_preds = np.mean([test_preds_3_3_head, test_preds_3_3_tail, test_preds_3_3_mix], 0)\n",
    "\n",
    "time_elapsed = time.time() - t0\n",
    "print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertCatLastThreeClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_7 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH7+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_7.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds_7 = np.mean(test_preds_7, 0)\n",
    "# test_preds = np.mean(test_preds_7, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(\"Building model...\")\n",
    "# model = BertLastThreeClassification(bert_config, num_labels=30)\n",
    "# model = model.to(device)\n",
    "\n",
    "# # test inference\n",
    "# t0 = time.time()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# test_preds_8 = []\n",
    "# for i in range(5):\n",
    "#     models = torch.load(MODEL_PATH8+f'fld_{i}_quest_bert_models.pt')['model']\n",
    "#     model.load_state_dict(models[list(models.keys())[-1]])\n",
    "#     test_preds_8.append(eval_model(model, test_loader)[test_original_indices])\n",
    "# # test_preds_8 = np.mean(test_preds_8, 0)\n",
    "# test_preds = np.mean(test_preds_8, 0)\n",
    "\n",
    "# time_elapsed = time.time() - t0\n",
    "# print('time = {:.0f}m {:.0f}s.'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_preds = test_preds_1*0.1 + test_preds_2*0.2 + test_preds_3*0.7 + test_preds_4*0.7 + test_preds_5*0.7\n",
    "# test_preds = test_preds_2*0.21 + test_preds_3*0.3 + test_preds_4*0.4 + test_preds_5*0.09\n",
    "# test_preds = test_preds_1*0.3 + test_preds_2*0.7\n",
    "\n",
    "# test_preds = test_preds_3*0.4 + test_preds_3_1*0.2 + test_preds_3_2*0.2 + test_preds_3_3*0.2\n",
    "# test_preds = test_preds_2*0.3 + test_preds_3_1*0.3 + test_preds_3*0.4\n",
    "\n",
    "# test_preds = test_preds_3*0.5 + test_preds_6*0.5\n",
    "# test_preds = test_preds_3*0.4 + test_preds_3_1*0.3 + test_preds_6*0.3\n",
    "test_preds = test_preds_3_3*0.3 + test_preds_3*0.25 + test_preds_3_1*0.2 + test_preds_6*0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_voters = [142, 30, 6, 33, 13, 4, 86, 23, 7, 142, \\\n",
    "              15, 9, 6, 9, 8, 7, 5, 50, 36, 149, \\\n",
    "              52, 24, 49, 81, 88, 64, 17, 54, 50, 127]\n",
    "\n",
    "for i in range(test_preds.shape[1]):\n",
    "    test_preds[:,i]= (test_preds[:,i] // (1/max_voters[i])) * (1/max_voters[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sakami's method of post-processing: golden section search\n",
    "# # https://www.kaggle.com/c/google-quest-challenge/discussion/129927\n",
    "\n",
    "# from typing import Dict, List, Tuple\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# class OptimizedRounder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.threshold = [0., 1.]\n",
    "#         self.ab_start = [(0., 0.2), (0.8, 1.)]\n",
    "\n",
    "#     def fit(self, train_labels, train_preds):\n",
    "#         assert train_labels.shape == train_preds.shape\n",
    "#         assert train_labels.ndim == 1\n",
    "\n",
    "#         self.best_score = self.score(train_labels, train_preds)\n",
    "#         self._golden_section_search(train_labels, train_preds, 0)  # lower threshold\n",
    "#         score = self.score(train_labels, train_preds)\n",
    "#         if score > self.best_score + 1e-3:\n",
    "#             self.best_score = score\n",
    "#         else:\n",
    "#             self.threshold[0] = 0.\n",
    "\n",
    "#         self._golden_section_search(train_labels, train_preds, 1)  # higher threshold\n",
    "#         score = self.score(train_labels, train_preds)\n",
    "#         if score > self.best_score + 1e-3:\n",
    "#             self.best_score = score\n",
    "#         else:\n",
    "#             self.threshold[1] = 1.\n",
    "\n",
    "#     def _golden_section_search(self, train_labels, train_preds, idx):\n",
    "#         # idx == 0 -> lower threshold search\n",
    "#         # idx == 1 -> higher threshold search\n",
    "#         golden1 = 0.618\n",
    "#         golden2 = 1 - golden1\n",
    "#         for _ in range(10):\n",
    "#             a, b = self.ab_start[idx]\n",
    "#             # calc losses\n",
    "#             self.threshold[idx] = a\n",
    "#             la = -self.score(train_labels, train_preds)\n",
    "#             self.threshold[idx] = b\n",
    "#             lb = -self.score(train_labels, train_preds)\n",
    "#             for _ in range(20):\n",
    "#                 # choose value\n",
    "#                 if la > lb:\n",
    "#                     a = b - (b - a) * golden1\n",
    "#                     self.threshold[idx] = a\n",
    "#                     la = -self.score(train_labels, train_preds)\n",
    "#                 else:\n",
    "#                     b = b - (b - a) * golden2\n",
    "#                     self.threshold[idx] = b\n",
    "#                     lb = -self.score(train_labels, train_preds)\n",
    "\n",
    "#     def transform(self, preds):\n",
    "#         transformed = np.clip(preds, *self.threshold)\n",
    "#         if np.unique(transformed).size == 1:\n",
    "#             return preds\n",
    "#         return transformed\n",
    "\n",
    "#     def score(self, labels, preds):\n",
    "#         p = self.transform(preds)\n",
    "#         score = spearmanr(labels, p)[0]\n",
    "#         return score\n",
    "    \n",
    "\n",
    "# def get_scores(y_true, y_pred) -> Dict[str, float]:\n",
    "#     # y_true, y_pred: np.ndarray with shape (sample_size, num_targets)\n",
    "#     assert y_true.shape == y_pred.shape\n",
    "#     scores = {}\n",
    "#     for i, target_name in enumerate(train_df.columns[11:]):\n",
    "#         scores[target_name] = spearmanr(y_true[:, i], y_pred[:, i])[0]\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds = \\\n",
    "# defaultdict(list,\n",
    "#             {'question_asker_intent_understanding': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'question_body_critical': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'question_conversational': [[0.15179361491144003, 1.0],\n",
    "#               [0.1526515002805198, 1.0],\n",
    "#               [0.1526515002805198, 1.0]],\n",
    "#              'question_expect_short_answer': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'question_fact_seeking': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'question_has_commonly_accepted_answer': [[0.0,\n",
    "#                0.8000132068576686],\n",
    "#               [0.0, 0.8052740200518711],\n",
    "#               [0.0, 0.8001597425885231]],\n",
    "#              'question_interestingness_others': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'question_interestingness_self': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'question_multi_intent': [[0.14753784054476035, 1.0],\n",
    "#               [0.13778275878897764, 1.0],\n",
    "#               [0.12290187523643346, 1.0]],\n",
    "#              'question_not_really_a_question': [[0.013759602164305064, 1.0],\n",
    "#               [0.01802086694133006, 1.0],\n",
    "#               [0.00687251255016287, 1.0]],\n",
    "#              'question_opinion_seeking': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'question_type_choice': [[0.12734813717513502, 1.0],\n",
    "#               [0.12988982368753088, 1.0],\n",
    "#               [0.12362481850246866, 1.0]],\n",
    "#              'question_type_compare': [[0.15529071537963646, 1.0],\n",
    "#               [0.15529071537963646, 1.0],\n",
    "#               [0.16294273169729784, 1.0]],\n",
    "#              'question_type_consequence': [[0.08057223398703053, 1.0],\n",
    "#               [0.11797798670613772, 1.0],\n",
    "#               [0.12013096495104046, 1.0]],\n",
    "#              'question_type_definition': [[0.168204391758552, 1.0],\n",
    "#               [0.1278810398840085, 1.0],\n",
    "#               [0.168204391758552, 1.0]],\n",
    "#              'question_type_entity': [[0.14588181969092245, 1.0],\n",
    "#               [0.12726306722893924, 1.0],\n",
    "#               [0.19575783226362878, 1.0]],\n",
    "#              'question_type_instructions': [[0.12299246347539239,\n",
    "#                0.8107715462573686],\n",
    "#               [0.07377177371375203, 0.8083782962560995],\n",
    "#               [0.05334678194812586, 0.8101507557008114]],\n",
    "#              'question_type_procedure': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'question_type_reason_explanation': [[0.09831179170683711, 1.0],\n",
    "#               [0.15280740045766866, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'question_type_spelling': [[0.009473640174145823, 1.0],\n",
    "#               [0.018044454423497253, 1.0],\n",
    "#               [0.018044454423497253, 1.0]],\n",
    "#              'question_well_written': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'answer_helpful': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'answer_level_of_information': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'answer_plausible': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'answer_relevance': [[0.0, 1.0],\n",
    "#               [0.0, 0.9555842698655993],\n",
    "#               [0.0, 0.98346289268097]],\n",
    "#              'answer_satisfaction': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'answer_type_instructions': [[0.069749400282935, 1.0],\n",
    "#               [0.10554600900728257, 0.8946810438377845],\n",
    "#               [0.055883473523577906, 0.8840116932937044]],\n",
    "#              'answer_type_procedure': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]],\n",
    "#              'answer_type_reason_explanation': [[0.0, 1.0],\n",
    "#               [0.0, 1.0],\n",
    "#               [0.0, 1.0]],\n",
    "#              'answer_well_written': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]})\n",
    "\n",
    "\n",
    "# for i, target_name in enumerate(output_cols):\n",
    "#     optimizer = OptimizedRounder()\n",
    "#     optimizer.threshold = np.mean(thresholds[target_name], axis=0)\n",
    "#     test_preds[:, i] = optimizer.transform(test_preds[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Post processing of column 'question_type_spelling'\n",
    "\n",
    "# col_ind = np.where(sub_df.columns[1:] == 'question_type_spelling')[0][0]\n",
    "# post_idx = np.where((test_df.category == \"CULTURE\") & \\\n",
    "#                     ((test_df.host == \"english.stackexchange.com\") | (test_df.host == \"ell.stackexchange.com\")))[0]\n",
    "\n",
    "# pred_post = np.zeros(len(sub_df))\n",
    "# pred_post[post_idx] += 0.5\n",
    "\n",
    "# test_preds[:, col_ind] = pred_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Post processing\n",
    "\n",
    "# col_ind = np.where(sub_df.columns[1:] == 'answer_helpful')[0][0]\n",
    "# col_rel_ind = np.where(sub_df.columns[1:] == 'answer_satisfaction')[0][0]\n",
    "# col_rel_2_ind = np.where(sub_df.columns[1:] == 'answer_relevance')[0][0]\n",
    "# test_preds[:, col_ind] = 0.333*(test_preds[:, col_ind] + test_preds[:, col_rel_ind] + test_preds[:, col_rel_2_ind])\n",
    "\n",
    "\n",
    "# col_ind = np.where(sub_df.columns[1:] == 'answer_plausible')[0][0]\n",
    "# col_rel_ind = np.where(sub_df.columns[1:] == 'answer_helpful')[0][0]\n",
    "# test_preds[:, col_ind] = 0.5*(test_preds[:, col_ind] + test_preds[:, col_rel_ind])\n",
    "\n",
    "\n",
    "# col_ind = np.where(sub_df.columns[1:] == 'answer_relevance')[0][0]\n",
    "# col_rel_ind = np.where(sub_df.columns[1:] == 'answer_satisfaction')[0][0]\n",
    "# col_rel_2_ind = np.where(sub_df.columns[1:] == 'answer_helpful')[0][0]\n",
    "# test_preds[:, col_ind] = 0.333*(test_preds[:, col_ind] + test_preds[:, col_rel_ind] + test_preds[:, col_rel_2_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = train_df[output_cols].values\n",
    "\n",
    "# for column_ind in range(30):\n",
    "#     curr_column = y_train[:, column_ind]\n",
    "#     values = np.unique(curr_column)\n",
    "#     map_quantiles = []\n",
    "#     for val in values:\n",
    "#         occurrence = np.mean(curr_column == val)\n",
    "#         cummulative = sum(el['occurrence'] for el in map_quantiles)\n",
    "#         map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n",
    "            \n",
    "#     for quant in map_quantiles:\n",
    "#         pred_col = test_preds[:, column_ind]\n",
    "#         q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n",
    "#         pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n",
    "#         test_preds[:, column_ind] = pred_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878.861312M\n",
      "2394.947584M\n"
     ]
    }
   ],
   "source": [
    "print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.iloc[:, 1:] = test_preds\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
