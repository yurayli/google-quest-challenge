{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pip/_internal/commands/install.py:243: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\r\n",
      "  cmdoptions.check_install_build_global(options)\r\n",
      "Processing /kaggle/input/nvidia-apex/apex-880ab92\r\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\r\n",
      "Installing collected packages: apex\r\n",
      "  Running setup.py install for apex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25hSuccessfully installed apex-0.1\r\n"
     ]
    }
   ],
   "source": [
    "# Installing Nvidia Apex\n",
    "! pip install -q -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /kaggle/input/nvidia-apex/apex-880ab92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os, sys, re, gc, pickle, operator, shutil, copy, random\n",
    "import time, datetime\n",
    "\n",
    "from math import floor, ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, Sampler\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, BertConfig\n",
    "from pytorch_pretrained_bert.modeling import BertModel, BertPreTrainedModel\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/google-quest-challenge/\"\n",
    "BERT_MODEL_PATH = '/kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "output_model_file = \"quest_bert_models.pt\"\n",
    "\n",
    "SEED = 2019\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "epochs_for_sched = 3 #5\n",
    "checkpoint_iter = 939 #608\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "warmup_proportion = 0.2\n",
    "grad_accumulation_steps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed for randomness in pytorch\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of data preprocessing and pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_and_tokenize(title, question, answer, max_sequence_length, tokenizer,\n",
    "                      trunc_mode='head', t_max_len=18, q_max_len=245, a_max_len=244):\n",
    "    \n",
    "    assert trunc_mode in {\"head\", \"tail\", \"mix\"}\n",
    "    need_trunc = False\n",
    "\n",
    "    tq_sep = tokenizer.tokenize(\"Details:\")\n",
    "    t = tokenizer.tokenize(title)\n",
    "    q = tokenizer.tokenize(question)\n",
    "    a = tokenizer.tokenize(answer)\n",
    "    \n",
    "    t_len = len(t)\n",
    "    q_len = len(q)\n",
    "    a_len = len(a)\n",
    "\n",
    "    if (t_len+q_len+a_len+5) > max_sequence_length:\n",
    "        need_trunc = True\n",
    "        \n",
    "        if t_max_len > t_len:\n",
    "            t_new_len = t_len\n",
    "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
    "        else:\n",
    "            t_new_len = t_max_len\n",
    "      \n",
    "        if a_max_len > a_len:\n",
    "            a_new_len = a_len \n",
    "            q_new_len = q_max_len + (a_max_len - a_len)\n",
    "        elif q_max_len > q_len:\n",
    "            a_new_len = a_max_len + (q_max_len - q_len)\n",
    "            q_new_len = q_len\n",
    "        else:\n",
    "            a_new_len = a_max_len\n",
    "            q_new_len = q_max_len\n",
    "            \n",
    "        if t_new_len+a_new_len+q_new_len+5 != max_sequence_length:\n",
    "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
    "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+5)))\n",
    "        \n",
    "        if trunc_mode == \"head\":\n",
    "            t = t[:t_new_len]\n",
    "            q = q[:q_new_len]\n",
    "            a = a[:a_new_len]\n",
    "        if trunc_mode == \"tail\":\n",
    "            t = t[-t_new_len:]\n",
    "            q = q[-q_new_len:]\n",
    "            a = a[-a_new_len:]\n",
    "        if trunc_mode == \"mix\":\n",
    "            def trunc_seq(seq, seq_max_len, trunc_ratio=0.6):\n",
    "                maj_len = int(seq_max_len * trunc_ratio)\n",
    "                return seq[:maj_len] + seq[-(seq_max_len-maj_len):]\n",
    "            t = trunc_seq(t, t_new_len)\n",
    "            q = trunc_seq(q, q_new_len)\n",
    "            a = trunc_seq(a, a_new_len)\n",
    "    \n",
    "    return t, tq_sep, q, a, need_trunc\n",
    "\n",
    "\n",
    "# Tokenizing the lines to BERT token\n",
    "def convert_lines(df, columns, max_sequence_length, tokenizer, trunc_mode='head',\n",
    "                  misc_trunc=False, target=None, sample_weighting=False):\n",
    "    all_tokens = []\n",
    "    segment_ids = []   # representing segmentation of sentence A and B\n",
    "    if target is not None:\n",
    "        labels = []\n",
    "    \n",
    "    for ind, (_, instance) in enumerate(df[columns].iterrows()):\n",
    "        \n",
    "        title, question, answer = instance.question_title, instance.question_body, instance.answer\n",
    "        t, tq_sep, q, a, need_trunc = trim_and_tokenize(title, question, answer,\n",
    "                                                        max_sequence_length, tokenizer, trunc_mode=trunc_mode)\n",
    "        tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "        all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "        segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "        if target is not None:\n",
    "            if sample_weighting:\n",
    "                labels.append(np.concatenate([target[ind], [3./7]]))\n",
    "            else:\n",
    "                labels.append(target[ind])\n",
    "        \n",
    "        if need_trunc and misc_trunc:\n",
    "            t, tq_sep, q, a, _ = trim_and_tokenize(title, question, answer,\n",
    "                                                   max_sequence_length, tokenizer, trunc_mode='tail')\n",
    "            tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "            all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "\n",
    "            t, tq_sep, q, a, _ = trim_and_tokenize(title, question, answer,\n",
    "                                                   max_sequence_length, tokenizer, trunc_mode='mix')\n",
    "            tokens = [\"[CLS]\"] + t + tq_sep + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "            all_tokens.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "            segment_ids.append([0]*(len(t)+len(tq_sep)+len(q)+2) + [1]*(len(a)+1))\n",
    "\n",
    "            if target is not None:\n",
    "                if sample_weighting:\n",
    "                    labels.extend([np.concatenate([target[ind], [2./7]])] * 2)\n",
    "                else:\n",
    "                    labels.extend([target[ind]] * 2)\n",
    "    \n",
    "    if target is not None:\n",
    "        return np.array(all_tokens), np.array(segment_ids), np.array(labels)\n",
    "    return np.array(all_tokens), np.array(segment_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset and dataloader\n",
    "\n",
    "class QuestQAs(Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_comments, segment_ids, targets=None, split=None, maxlen=256):\n",
    "        self.comments = tokenized_comments\n",
    "        self.segment_ids = segment_ids\n",
    "        self.targets = targets\n",
    "        self.split = split\n",
    "        assert self.split in {'train', 'valid', 'test'}\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.comments[index]\n",
    "        segment_id = self.segment_ids[index]\n",
    "        if self.targets is not None:\n",
    "            target = self.targets[index]\n",
    "            return comment, segment_id, torch.FloatTensor(target)\n",
    "        else:\n",
    "            return comment, segment_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def get_lens(self):\n",
    "        lengths = np.fromiter(\n",
    "            ((min(self.maxlen, len(seq))) for seq in self.comments),\n",
    "            dtype=np.int32)\n",
    "        return lengths\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for sequence bucketing\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "\n",
    "        :param batch: an iterable of N sets from __getitem__()\n",
    "        :return: a tensor of comments, and targets\n",
    "        \"\"\"\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            comments, segment_ids, targets = zip(*batch)\n",
    "        else:\n",
    "            comments, segment_ids = zip(*batch)\n",
    "\n",
    "        lengths = [len(c) for c in comments]\n",
    "        maxlen = max(lengths)\n",
    "        padded_comments, padded_seg_ids = [], []\n",
    "        for i, (c, s) in enumerate(zip(comments, segment_ids)):\n",
    "            padded_comments.append(c+[0]*(maxlen - lengths[i]))\n",
    "            padded_seg_ids.append(s +[0]*(maxlen - lengths[i]))\n",
    "\n",
    "        if self.split in ('train', 'valid'):\n",
    "            return torch.LongTensor(padded_comments), torch.LongTensor(padded_seg_ids), torch.stack(targets)\n",
    "        else:\n",
    "            return torch.LongTensor(padded_comments), torch.LongTensor(padded_seg_ids)\n",
    "\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "\n",
    "    def __init__(self, data_source, sort_lens, bucket_size=None, batch_size=1024, shuffle_data=True):\n",
    "        super().__init__(data_source)\n",
    "        self.shuffle = shuffle_data\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_lens = sort_lens\n",
    "        self.bucket_size = bucket_size if bucket_size is not None else len(sort_lens)\n",
    "        self.weights = None\n",
    "\n",
    "        if not shuffle_data:\n",
    "            self.index = self.prepare_buckets()\n",
    "        else:\n",
    "            self.index = None\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        assert weights >= 0\n",
    "        total = np.sum(weights)\n",
    "        if total != 1:\n",
    "            weights = weights / total\n",
    "        self.weights = weights\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = None\n",
    "        if self.weights is not None:\n",
    "            total = len(self.sort_lens)\n",
    "            indices = np.random.choice(total, (total,), p=self.weights)\n",
    "        if self.shuffle:\n",
    "            self.index = self.prepare_buckets(indices)\n",
    "        return iter(self.index)\n",
    "\n",
    "    def get_reverse_indexes(self):\n",
    "        indexes = np.zeros((len(self.index),), dtype=np.int32)\n",
    "        for i, j in enumerate(self.index):\n",
    "            indexes[j] = i\n",
    "        return indexes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sort_lens)\n",
    "\n",
    "    def prepare_buckets(self, indices=None):\n",
    "        lengths = - self.sort_lens\n",
    "        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lengths)\n",
    "\n",
    "        if indices is None:\n",
    "            if self.shuffle:\n",
    "                indices = shuffle(np.arange(len(lengths), dtype=np.int32))\n",
    "                lengths = lengths[indices]\n",
    "            else:\n",
    "                indices = np.arange(len(lengths), dtype=np.int32)\n",
    "\n",
    "        #  bucket iterator\n",
    "        def divide_chunks(l, n):\n",
    "            if n == len(l):\n",
    "                yield np.arange(len(l), dtype=np.int32), l\n",
    "            else:\n",
    "                # looping till length l\n",
    "                for i in range(0, len(l), n):\n",
    "                    data = l[i:i + n]\n",
    "                    yield np.arange(i, i + len(data), dtype=np.int32), data\n",
    "\n",
    "        new_indices = []\n",
    "        extra_batch_idx = None\n",
    "        for chunk_index, chunk in divide_chunks(lengths, self.bucket_size):\n",
    "            # sort indices in bucket by descending order of length\n",
    "            indices_sorted = chunk_index[np.argsort(chunk)]\n",
    "\n",
    "            batch_idxes = []\n",
    "            for _, batch_idx in divide_chunks(indices_sorted, self.batch_size):\n",
    "                if len(batch_idx) == self.batch_size:\n",
    "                    batch_idxes.append(batch_idx.tolist())\n",
    "                else:\n",
    "                    assert extra_batch_idx is None\n",
    "                    assert batch_idx is not None\n",
    "                    extra_batch_idx = batch_idx.tolist()\n",
    "\n",
    "            # shuffling batches within buckets\n",
    "            if self.shuffle:\n",
    "                batch_idxes = shuffle(batch_idxes)\n",
    "            for batch_idx in batch_idxes:\n",
    "                new_indices.extend(batch_idx)\n",
    "\n",
    "        if extra_batch_idx is not None:\n",
    "            new_indices.extend(extra_batch_idx)\n",
    "\n",
    "        if not self.shuffle:\n",
    "            self.original_indices = np.argsort(indices_sorted).tolist()\n",
    "        return indices[new_indices]\n",
    "\n",
    "\n",
    "def prepare_loader(x, seg_ids, y=None, batch_size=None, split=None):\n",
    "    assert split in {'train', 'valid', 'test'}\n",
    "    dataset = QuestQAs(x, seg_ids, y, split, MAX_SEQUENCE_LENGTH)\n",
    "    if split == 'train':\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                bucket_size=batch_size*20, batch_size=batch_size)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn)\n",
    "    else:\n",
    "        sampler = BucketSampler(dataset, dataset.get_lens(),\n",
    "                                batch_size=batch_size, shuffle_data=False)\n",
    "        return DataLoader(dataset, batch_size=batch_size, sampler=sampler,\n",
    "                          collate_fn=dataset.collate_fn), sampler.original_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions of metrics, lr range test, training and logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "def compute_rho(labels, preds):\n",
    "    rhos = []\n",
    "    for col_label, col_pred in zip(labels.T, preds.T):\n",
    "        rhos.append(\n",
    "            spearmanr(col_label, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n",
    "    return np.mean(rhos)\n",
    "\n",
    "\n",
    "# Functions for the training process\n",
    "class NetSolver(object):\n",
    "\n",
    "    def __init__(self, model, criterion, optimizer, scheduler, print_freq, filepath):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.print_freq = print_freq\n",
    "        self.filepath = filepath\n",
    "\n",
    "        self.model = self.model.to(device)\n",
    "        self.criterion = self.criterion.to(device)\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        \"\"\"Set up some book-keeping variables for optimization.\n",
    "        \"\"\"\n",
    "        self.best_val_loss = 1e4\n",
    "        self.best_val_rho = 0.\n",
    "        self.loss_history = []\n",
    "        self.val_loss_history = []\n",
    "        self.rho_history = []\n",
    "        self.val_rho_history = []\n",
    "        self.val_preds = []\n",
    "        self.models = {}\n",
    "\n",
    "    def save_checkpoint(self, iteration):\n",
    "        \"\"\"Save model checkpoint.\n",
    "        \"\"\"\n",
    "#         self.models[f'ckpt_{iteration}'] = self.model.state_dict()\n",
    "        self.models['ckpt_best'] = self.model.state_dict()\n",
    "    \n",
    "    def save_final_state(self):\n",
    "        \"\"\"Save final states.\n",
    "        \"\"\"\n",
    "        state = {'model': self.models,\n",
    "#                  'optimizer': self.optimizer.state_dict()\n",
    "                 'optimizer': None\n",
    "                 }\n",
    "        torch.save(state, self.filepath)\n",
    "\n",
    "    def forward_pass(self, x, seg_ids, y):\n",
    "        \"\"\"Forward pass through the network.\n",
    "        \"\"\"\n",
    "        x, y = x.to(device=device, dtype=torch.long), y.to(device=device, dtype=torch.float)\n",
    "        seg_ids = seg_ids.to(device=device, dtype=torch.long)\n",
    "        scores = self.model(x, token_type_ids=seg_ids, attention_mask=(x>0))\n",
    "        loss = self.criterion(scores, y)\n",
    "        return loss, torch.sigmoid(scores)\n",
    "\n",
    "    def train(self, loaders, iterations, start_time):\n",
    "        \"\"\"Weight of network updated by apex, grad accumulation, model checkpoint.\n",
    "        \"\"\"\n",
    "        train_loader, val_loader = loaders\n",
    "        loader = iter(train_loader)\n",
    "        running_loss = 0.\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # start training for iterations\n",
    "        for i in range(iterations):\n",
    "            self.model.train()\n",
    "\n",
    "            try:\n",
    "                x, seg_ids, y = next(loader)\n",
    "            except:  # after an loader running out\n",
    "                loader = iter(train_loader)\n",
    "                x, seg_ids, y = next(loader)\n",
    "            loss, _ = self.forward_pass(x, seg_ids, y)\n",
    "            \n",
    "#             loss.backward()\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "\n",
    "            # gradient accumulation for larger batch size effect with smaller memory usage\n",
    "            if (i+1) % grad_accumulation_steps == 0:   # Wait for several backward steps\n",
    "                self.optimizer.step()                  # Now we can do an optimizer step\n",
    "                self.optimizer.zero_grad()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # verbose and checkpoint\n",
    "            if (i+1) % self.print_freq == 0 or (i+1) == iterations:\n",
    "                print(f'Iteration {i+1}:')\n",
    "                train_rho, _ = self.check_auc(train_loader, num_batches=50)\n",
    "                print('{\"metric\": \"Loss\", \"value\": %.4f}' % (running_loss/(i+1),))\n",
    "                print('{\"metric\": \"Rho\", \"value\": %.4f}' % (train_rho,))\n",
    "                \n",
    "                val_rho, val_loss, val_scores = self.check_auc(val_loader)\n",
    "                print('{\"metric\": \"Val. Loss\", \"value\": %.4f}' % (val_loss,))\n",
    "                print('{\"metric\": \"Val. Rho\", \"value\": %.4f}' % (val_rho,))\n",
    "                \n",
    "                self.loss_history.append(running_loss/(i+1))\n",
    "                self.val_loss_history.append(val_loss)\n",
    "                self.rho_history.append(train_rho)\n",
    "                self.val_rho_history.append(val_rho)\n",
    "                self.val_preds.append(val_scores)\n",
    "                \n",
    "                if val_loss < self.best_val_loss:\n",
    "                    print('updating best val loss...')\n",
    "                    self.best_val_loss = val_loss\n",
    "                if val_rho > self.best_val_rho:\n",
    "                    print('updating best val Spearman R...')\n",
    "                    self.best_val_rho = val_rho\n",
    "                    self.save_checkpoint(i+1)\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                print()\n",
    "                \n",
    "            if (time.time() - start_time) > 29000:\n",
    "                break\n",
    "        \n",
    "        self.save_final_state()\n",
    "\n",
    "    def check_auc(self, loader, num_batches=None):\n",
    "        \"\"\"Calculate metrics for validation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        targets, scores, losses = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for t, (x, seg_ids, y) in enumerate(loader):\n",
    "                l, score = self.forward_pass(x, seg_ids, y)\n",
    "                targets.append(y.cpu().numpy())\n",
    "                scores.append(score.cpu().numpy())\n",
    "                losses.append(l.item())\n",
    "                if num_batches is not None and (t+1) == num_batches:\n",
    "                    break\n",
    "\n",
    "        targets = np.concatenate(targets)\n",
    "        scores = np.concatenate(scores)\n",
    "        rho = compute_rho(targets, scores)\n",
    "        loss = np.mean(losses)\n",
    "        \n",
    "        if num_batches is None:\n",
    "            return rho, loss, scores\n",
    "        return rho, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr range test and schedule\n",
    "\n",
    "class OneCycleScheduler(object):\n",
    "    # one-cycle scheduler\n",
    "    SCHEDULES = set(['cosine', 'linear', 'linear_cosine'])\n",
    "\n",
    "    def __init__(self, optimizer, iterations, sched_profile='cosine', max_lr=3e-3,\n",
    "                 moms=(.95, .85), div_factor=25, warmup=0.3, final_div=None):\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        assert sched_profile in self.SCHEDULES\n",
    "        self.sched_profile = sched_profile\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "            self.init_lrs = [lr/div_factor for lr in self.max_lrs]\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "            self.init_lrs = [max_lr/div_factor] * len(optimizer.param_groups)\n",
    "\n",
    "        self.final_div = final_div\n",
    "        if self.final_div is None: self.final_div = div_factor*1e4\n",
    "        self.final_lrs = [lr/self.final_div for lr in self.max_lrs]\n",
    "        self.moms = moms\n",
    "\n",
    "        self.total_iteration = iterations\n",
    "        self.up_iteration = int(self.total_iteration * warmup)\n",
    "        self.down_iteration = self.total_iteration - self.up_iteration\n",
    "\n",
    "        self.curr_iter = 0\n",
    "        self._assign_lr_mom(self.init_lrs, [moms[0]]*len(optimizer.param_groups))\n",
    "\n",
    "    def _assign_lr_mom(self, lrs, moms):\n",
    "        for param_group, lr, mom in zip(self.optimizer.param_groups, lrs, moms):\n",
    "            param_group['lr'] = lr\n",
    "            param_group['betas'] = (mom, 0.999)\n",
    "\n",
    "    def _annealing_cos(self, start, end, pct):\n",
    "        cos_out = np.cos(np.pi * pct) + 1\n",
    "        return end + (start-end)/2 * cos_out\n",
    "\n",
    "    def _annealing_linear(self, start, end, pct):\n",
    "        return start + pct * (end-start)\n",
    "    \n",
    "    def _annealing_function(self, curr_iter):\n",
    "        if self.sched_profile == 'cosine':\n",
    "            return self._annealing_cos\n",
    "        if self.sched_profile == 'linear':\n",
    "            return self._annealing_linear\n",
    "        if self.sched_profile == 'linear_cosine':\n",
    "            if curr_iter <= self.up_iteration:\n",
    "                return self._annealing_linear\n",
    "            else:\n",
    "                return self._annealing_cos\n",
    "    \n",
    "    def step(self):\n",
    "        self.curr_iter += 1\n",
    "        anneal = self._annealing_function(self.curr_iter)\n",
    "\n",
    "        if self.curr_iter <= self.up_iteration:\n",
    "            pct = self.curr_iter / self.up_iteration\n",
    "            curr_lrs = [anneal(min_lr, max_lr, pct) \\\n",
    "                            for min_lr, max_lr in zip(self.init_lrs, self.max_lrs)]\n",
    "            curr_moms = [anneal(self.moms[0], self.moms[1], pct) \\\n",
    "                            for _ in range(len(self.optimizer.param_groups))]\n",
    "        else:\n",
    "            pct = (self.curr_iter-self.up_iteration) / self.down_iteration\n",
    "            curr_lrs = [anneal(max_lr, final_lr, pct) \\\n",
    "                            for max_lr, final_lr in zip(self.max_lrs, self.final_lrs)]\n",
    "            curr_moms = [anneal(self.moms[1], self.moms[0], pct) \\\n",
    "                            for _ in range(len(self.optimizer.param_groups))]\n",
    "\n",
    "        self._assign_lr_mom(curr_lrs, curr_moms)\n",
    "\n",
    "\n",
    "def lr_range_test(train_loader, model, optimizer, criterion, start_lr=1e-7,\n",
    "                  end_lr=10, num_it=100, stop_div=True):\n",
    "    epochs = int(np.ceil(num_it/len(train_loader)))\n",
    "    n_groups = len(optimizer.param_groups)\n",
    "\n",
    "    if isinstance(start_lr, list) or isinstance(start_lr, tuple):\n",
    "        if len(start_lr) != n_groups:\n",
    "            raise ValueError(\"expected {} max_lr, got {}\".format(n_groups, len(start_lr)))\n",
    "        start_lrs = list(start_lr)\n",
    "    else:\n",
    "        start_lrs = [start_lr] * n_groups\n",
    "\n",
    "    if isinstance(end_lr, list) or isinstance(end_lr, tuple):\n",
    "        if len(end_lr) != n_groups:\n",
    "            raise ValueError(\"expected {} max_lr, got {}\".format(n_groups, len(end_lr)))\n",
    "        end_lrs = list(end_lr)\n",
    "    else:\n",
    "        end_lrs = [end_lr] * n_groups\n",
    "\n",
    "    curr_lrs = start_lrs*1\n",
    "    for param_group, lr in zip(optimizer.param_groups, curr_lrs):\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    n, lrs_logs, loss_log = 0, [], []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.train()\n",
    "        for x, seg_ids, y in train_loader:\n",
    "            x, y = x.to(device=device, dtype=torch.long), y.to(device=device, dtype=torch.float)\n",
    "            seg_ids = seg_ids.to(device=device, dtype=torch.long)\n",
    "            scores = model(x, token_type_ids=seg_ids, attention_mask=(x>0))\n",
    "            loss = criterion(scores, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lrs_logs.append(curr_lrs)\n",
    "            loss_log.append(loss.item())\n",
    "\n",
    "            # update best loss\n",
    "            if n == 0:\n",
    "                best_loss, n_best = loss.item(), n\n",
    "            else:\n",
    "                if loss.item() < best_loss:\n",
    "                    best_loss, n_best = loss.item(), n\n",
    "\n",
    "            # update lr per iter with exponential schedule\n",
    "            n += 1\n",
    "            curr_lrs = [lr * (end_lr/lr) ** (n/num_it) for lr, end_lr in zip(start_lrs, end_lrs)]\n",
    "            for param_group, lr in zip(optimizer.param_groups, curr_lrs):\n",
    "                param_group['lr'] = lr\n",
    "\n",
    "            # stopping condition\n",
    "            if n == num_it or (stop_div and (loss.item() > 4*best_loss or torch.isnan(loss))):\n",
    "                break\n",
    "\n",
    "    print('minimum loss {}, at lr {}'.format(best_loss, lrs_logs[n_best]))\n",
    "    return lrs_logs, loss_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building PyTorch model from configuration: {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Converting TensorFlow checkpoint from /kaggle/input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt\n",
      "Loading TF weight bert/embeddings/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/embeddings/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/embeddings/position_embeddings with shape [512, 768]\n",
      "Loading TF weight bert/embeddings/token_type_embeddings with shape [2, 768]\n",
      "Loading TF weight bert/embeddings/word_embeddings with shape [30522, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_0/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_0/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_1/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_1/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_10/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_10/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_11/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_11/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_2/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_2/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_3/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_3/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_4/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_4/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_5/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_5/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_6/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_6/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_7/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_7/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_8/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_8/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/output/dense/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/key/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/query/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/attention/self/value/kernel with shape [768, 768]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/bias with shape [3072]\n",
      "Loading TF weight bert/encoder/layer_9/intermediate/dense/kernel with shape [768, 3072]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/beta with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/bias with shape [768]\n",
      "Loading TF weight bert/encoder/layer_9/output/dense/kernel with shape [3072, 768]\n",
      "Loading TF weight bert/pooler/dense/bias with shape [768]\n",
      "Loading TF weight bert/pooler/dense/kernel with shape [768, 768]\n",
      "Loading TF weight cls/predictions/output_bias with shape [30522]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/beta with shape [768]\n",
      "Loading TF weight cls/predictions/transform/LayerNorm/gamma with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/bias with shape [768]\n",
      "Loading TF weight cls/predictions/transform/dense/kernel with shape [768, 768]\n",
      "Loading TF weight cls/seq_relationship/output_bias with shape [2]\n",
      "Loading TF weight cls/seq_relationship/output_weights with shape [2, 768]\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'position_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'token_type_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'embeddings', 'word_embeddings']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['bert', 'pooler', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'beta']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'LayerNorm', 'gamma']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'bias']\n",
      "Initialize PyTorch weight ['cls', 'predictions', 'transform', 'dense', 'kernel']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_bias']\n",
      "Initialize PyTorch weight ['cls', 'seq_relationship', 'output_weights']\n",
      "Save PyTorch model to ./pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./bert_config.json'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classifier\n",
    "class BertForCustomClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super(BertForCustomClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(0.3) for _ in range(5)])\n",
    "        self.classifier = nn.Linear(config.hidden_size*4, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        encoded_layers, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=True)\n",
    "        seq_op1 = encoded_layers[-1]\n",
    "        seq_op2 = encoded_layers[-2]\n",
    "        avg_pool1 = torch.mean(seq_op1, 1)\n",
    "        max_pool1, _ = torch.max(seq_op1, 1)\n",
    "        avg_pool2 = torch.mean(seq_op2, 1)\n",
    "        max_pool2, _ = torch.max(seq_op2, 1)\n",
    "        pooled_output = torch.cat((avg_pool1, max_pool1, avg_pool2, max_pool2), 1)\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                h = self.classifier(dropout(pooled_output))\n",
    "            else:\n",
    "                h += self.classifier(dropout(pooled_output))\n",
    "        return h / len(self.dropouts)\n",
    "\n",
    "\n",
    "class TruncLoss(nn.Module):\n",
    "    def forward(self, pred_scores, labels):\n",
    "        loss = 0\n",
    "        for i in range(pred_scores.size(1)):\n",
    "            loss += nn.BCEWithLogitsLoss(weight=labels[:,-1])(pred_scores[:,i], labels[:,i])\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Build model and optimizer\n",
    "def model_optimizer_init(ft_lrs, num_labels=30):\n",
    "    print(\"Building model and optimizer...\")\n",
    "    pre_model = BertForSequenceClassification.from_pretrained('./', num_labels=num_labels)\n",
    "    bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n",
    "    model = BertForCustomClassification(bert_config, num_labels=num_labels)\n",
    "    model.bert = copy.deepcopy(pre_model.bert)\n",
    "\n",
    "    params_bert = list(model.bert.parameters())\n",
    "    params_cls = list(model.classifier.parameters())\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': params_bert, 'lr':ft_lrs[0]},\n",
    "        {'params': params_cls, 'lr':ft_lrs[1]}\n",
    "        ]\n",
    "    \n",
    "    optimizer = optim.AdamW(optimizer_grouped_parameters)\n",
    "    \n",
    "    return model, optimizer\n",
    "    \n",
    "\n",
    "# Translate model from tensorflow to pytorch\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "    BERT_MODEL_PATH + 'bert_config.json',\n",
    "    './pytorch_model.bin')\n",
    "\n",
    "# Save config files in the same path as pretrained model\n",
    "# for model reloading (inference, resume training etc.)\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', './bert_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 30])\n"
     ]
    }
   ],
   "source": [
    "def model_test():\n",
    "    x = torch.randint(10, (8, 256))\n",
    "    bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n",
    "    model = BertForCustomClassification(bert_config, num_labels=30)\n",
    "    print(model(x).size())\n",
    "\n",
    "model_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load/preprocess the data, and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(train_df):\n",
    "#     kf = GroupKFold(n_splits=5)\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "#     cv_indices = [(tr_idx, val_idx) for tr_idx, val_idx in kf.split(train_df.question_body, groups=train_df.question_body)]\n",
    "    cv_indices = [(tr_idx, val_idx) for tr_idx, val_idx in kf.split(train_df)]\n",
    "    return cv_indices\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    output_cols = list(train_df.columns[11:])\n",
    "    input_cols = list(train_df.columns[[1,2,5]])\n",
    "    \n",
    "    train_tars = train_df[output_cols].values.astype('float32')\n",
    "    \n",
    "    return train_tars, train_df, input_cols\n",
    "\n",
    "\n",
    "def load_and_preproc():\n",
    "    train_df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "    output_cols = list(train_df.columns[11:])\n",
    "    input_cols = list(train_df.columns[[1,2,5]])\n",
    "    \n",
    "    train_tars = train_df[output_cols].values.astype('float32')\n",
    "    train_seq, train_seg_ids = convert_lines(train_df, input_cols, MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "\n",
    "    return train_seq, train_seg_ids, train_tars, train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and tokenizing...\n",
      "tokenizing complete in 0 seconds.\n",
      "CPU times: user 183 ms, sys: 10.5 ms, total: 194 ms\n",
      "Wall time: 213 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, do_lower_case=True)\n",
    "\n",
    "t0 = time.time()\n",
    "print('Loading and tokenizing...')\n",
    "train_tars, train_df, input_cols = load_data()\n",
    "# train_seq, train_seg_ids, train_tars, train_df = load_and_preproc()\n",
    "cv_indices = train_val_split(train_df)\n",
    "# trn_idx, val_idx = cv_indices[0]\n",
    "print('tokenizing complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Building model and optimizer...\n",
      "Start training.\n",
      "Iteration 939:\n",
      "{\"metric\": \"Loss\", \"value\": 4.6865}\n",
      "{\"metric\": \"Rho\", \"value\": 0.4009}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.1796}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3773}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 1878:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in greater\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:877: RuntimeWarning: invalid value encountered in less\n",
      "  return (self.a < x) & (x < self.b)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/_distn_infrastructure.py:1831: RuntimeWarning: invalid value encountered in less_equal\n",
      "  cond2 = cond0 & (x <= self.a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"metric\": \"Loss\", \"value\": 4.4024}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 10.9962}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.4040}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2816:\n",
      "{\"metric\": \"Loss\", \"value\": 4.2267}\n",
      "{\"metric\": \"Rho\", \"value\": 0.5236}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0560}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.4049}\n",
      "updating best val Spearman R...\n",
      "\n",
      "Training fold-1 complete in 1063.8241765499115 seconds.\n",
      "1983.65184M\n",
      "2984.247296M\n",
      "\n",
      "Fold 2:\n",
      "Building model and optimizer...\n",
      "Start training.\n",
      "Iteration 939:\n",
      "{\"metric\": \"Loss\", \"value\": 4.8234}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.4360}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3508}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 1878:\n",
      "{\"metric\": \"Loss\", \"value\": 4.4810}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 10.9947}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3840}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2814:\n",
      "{\"metric\": \"Loss\", \"value\": 4.2854}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0175}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3927}\n",
      "updating best val Spearman R...\n",
      "\n",
      "Training fold-2 complete in 1070.602972984314 seconds.\n",
      "2866.016768M\n",
      "3888.119808M\n",
      "\n",
      "Fold 3:\n",
      "Building model and optimizer...\n",
      "Start training.\n",
      "Iteration 939:\n",
      "{\"metric\": \"Loss\", \"value\": 4.7739}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.3082}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3591}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 1878:\n",
      "{\"metric\": \"Loss\", \"value\": 4.4606}\n",
      "{\"metric\": \"Rho\", \"value\": 0.4651}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0137}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3971}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2817:\n",
      "{\"metric\": \"Loss\", \"value\": 4.2752}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0364}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.4031}\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2820:\n",
      "{\"metric\": \"Loss\", \"value\": 4.2749}\n",
      "{\"metric\": \"Rho\", \"value\": 0.5004}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0364}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.4030}\n",
      "\n",
      "Training fold-3 complete in 1100.351057767868 seconds.\n",
      "3789.355008M\n",
      "5712.642048M\n",
      "\n",
      "Fold 4:\n",
      "Building model and optimizer...\n",
      "Start training.\n",
      "Iteration 939:\n",
      "{\"metric\": \"Loss\", \"value\": 4.6877}\n",
      "{\"metric\": \"Rho\", \"value\": 0.3902}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.4683}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3512}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 1878:\n",
      "{\"metric\": \"Loss\", \"value\": 4.4143}\n",
      "{\"metric\": \"Rho\", \"value\": 0.4829}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 10.9708}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3803}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2784:\n",
      "{\"metric\": \"Loss\", \"value\": 4.2492}\n",
      "{\"metric\": \"Rho\", \"value\": 0.5361}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 10.9922}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3844}\n",
      "updating best val Spearman R...\n",
      "\n",
      "Training fold-4 complete in 1069.968695640564 seconds.\n",
      "4720.421376M\n",
      "6413.090816M\n",
      "\n",
      "Fold 5:\n",
      "Building model and optimizer...\n",
      "Start training.\n",
      "Iteration 939:\n",
      "{\"metric\": \"Loss\", \"value\": 4.8705}\n",
      "{\"metric\": \"Rho\", \"value\": nan}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.4764}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3540}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 1878:\n",
      "{\"metric\": \"Loss\", \"value\": 4.5288}\n",
      "{\"metric\": \"Rho\", \"value\": 0.4503}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0614}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3863}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Iteration 2802:\n",
      "{\"metric\": \"Loss\", \"value\": 4.3425}\n",
      "{\"metric\": \"Rho\", \"value\": 0.5011}\n",
      "{\"metric\": \"Val. Loss\", \"value\": 11.0128}\n",
      "{\"metric\": \"Val. Rho\", \"value\": 0.3959}\n",
      "updating best val loss...\n",
      "updating best val Spearman R...\n",
      "\n",
      "Training fold-5 complete in 1086.8101193904877 seconds.\n",
      "5599.770624M\n",
      "7629.438976M\n",
      "\n",
      "{\"metric\": \"OOF Val. Rho\", \"value\": 0.3922}\n"
     ]
    }
   ],
   "source": [
    "oof_preds = np.zeros_like(train_tars)\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(cv_indices):\n",
    "    print(f'Fold {i+1}:')\n",
    "    \n",
    "    x_train, seg_train, y_train = convert_lines(train_df.iloc[trn_idx], input_cols,\n",
    "                                                MAX_SEQUENCE_LENGTH, tokenizer,\n",
    "                                                misc_trunc=True, target=train_tars[trn_idx],\n",
    "                                                sample_weighting=True)\n",
    "    x_val, seg_val = convert_lines(train_df.iloc[val_idx], input_cols,\n",
    "                                   MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "    y_val = train_tars[val_idx]\n",
    "    y_val = np.hstack([y_val, np.ones(len(y_val), dtype='float32')[:,None]])\n",
    "#     seg_train, seg_val = train_seg_ids[trn_idx], train_seg_ids[val_idx]\n",
    "#     x_train, x_val = train_seq[trn_idx], train_seq[val_idx]\n",
    "#     y_train, y_val = train_tars[trn_idx], train_tars[val_idx]\n",
    "    \n",
    "    train_loader = prepare_loader(x_train, seg_train, y_train, batch_size, split='train')\n",
    "    val_loader, val_original_indices = prepare_loader(x_val, seg_val, y_val, 16, split='valid')\n",
    "    \n",
    "    num_train_steps = int(epochs_for_sched * len(train_loader) / grad_accumulation_steps)\n",
    "    ft_lrs = [0.8*lr, lr]\n",
    "    model, optimizer = model_optimizer_init(ft_lrs)\n",
    "    scheduler = OneCycleScheduler(optimizer, num_train_steps, sched_profile='linear', max_lr=ft_lrs,\n",
    "                                  div_factor=40, warmup=warmup_proportion)\n",
    "    model = model.to(device)\n",
    "    criterion = TruncLoss().to(device)\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "    solver = NetSolver(model, criterion, optimizer, scheduler, checkpoint_iter, f'fld_{i}_'+output_model_file)\n",
    "    \n",
    "    n_iter = num_train_steps * grad_accumulation_steps\n",
    "    print('Start training.')\n",
    "    t0 = time.time()\n",
    "    solver.train((train_loader, val_loader), n_iter, t0)\n",
    "    print(f'Training fold-{i+1} complete in {time.time()-t0} seconds.')\n",
    "    \n",
    "    oof_preds[val_idx] += solver.val_preds[-1][val_original_indices]\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "    print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')\n",
    "    print()\n",
    "    \n",
    "oof_rho = compute_rho(train_tars, oof_preds)\n",
    "print('{\"metric\": \"OOF Val. Rho\", \"value\": %.4f}' % (oof_rho,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # training preparation\n",
    "\n",
    "# x_train, seg_train, y_train = convert_lines(train_df.iloc[trn_idx], input_cols,\n",
    "#                                             MAX_SEQUENCE_LENGTH, tokenizer,\n",
    "#                                             misc_trunc=True, target=train_tars[trn_idx],\n",
    "#                                             sample_weighting=True)\n",
    "# x_val, seg_val = convert_lines(train_df.iloc[val_idx], input_cols,\n",
    "#                                MAX_SEQUENCE_LENGTH, tokenizer)\n",
    "# y_val = train_tars[val_idx]\n",
    "# y_val = np.hstack([y_val, np.ones(len(y_val), dtype='float32')[:,None]])\n",
    "# # seg_train, seg_val = train_seg_ids[trn_idx], train_seg_ids[val_idx]\n",
    "# # x_train, x_val = train_seq[trn_idx], train_seq[val_idx]\n",
    "# # y_train, y_val = train_tars[trn_idx], train_tars[val_idx]\n",
    "\n",
    "# train_loader = prepare_loader(x_train, seg_train, y_train, batch_size, split='train')\n",
    "# val_loader, val_original_indices = prepare_loader(x_val, seg_val, y_val, 16, split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# del train_seq, train_seg_ids, train_tars, train_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "# print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_lrs = [0.8*lr, lr]\n",
    "# model, optimizer = model_optimizer_init(ft_lrs)\n",
    "# model = model.to(device)\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "# lrs_logs, loss_log = lr_range_test(train_loader, model.to(device), optimizer, TruncLoss().to(device), \n",
    "#                                    start_lr=(8e-8,1e-7), end_lr=(8,10))\n",
    "# lrs_logs = list(zip(*lrs_logs))\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(16,5))\n",
    "\n",
    "# ax = fig.add_subplot(1,2,1)\n",
    "# ax.plot(lrs_logs[0][10:-3], loss_log[10:-3])\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_xlabel('Learning rate')\n",
    "# ax.set_ylabel('Loss')\n",
    "\n",
    "# ax1 = fig.add_subplot(1,2,2)\n",
    "# ax1.plot(lrs_logs[-1][10:-3], loss_log[10:-3])\n",
    "# ax1.set_xscale('log')\n",
    "# ax1.set_xlabel('Learning rate')\n",
    "# ax1.set_ylabel('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model setup\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# num_train_steps = int(epochs_for_sched * len(train_loader) / grad_accumulation_steps)\n",
    "# ft_lrs = [0.8*lr, lr]\n",
    "\n",
    "# model, optimizer = model_optimizer_init(ft_lrs)\n",
    "# scheduler = OneCycleScheduler(optimizer, num_train_steps, sched_profile='linear', max_lr=ft_lrs,\n",
    "#                               div_factor=40, warmup=warmup_proportion)\n",
    "\n",
    "# model = model.to(device)\n",
    "# criterion = TruncLoss().to(device)\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\", verbosity=0)\n",
    "# solver = NetSolver(model, criterion, optimizer, scheduler, checkpoint_iter, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "# print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_iter = num_train_steps * grad_accumulation_steps\n",
    "# print('Start training.')\n",
    "# t0 = time.time()\n",
    "# solver.train((train_loader, val_loader), n_iter, t0)\n",
    "# print('Training complete in {:.0f} seconds.'.format(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(str(torch.cuda.memory_allocated(device)/1e6 ) + 'M')\n",
    "# print(str(torch.cuda.memory_cached(device)/1e6 ) + 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_weights = [2**e for e in range(len(solver.val_preds))]\n",
    "# val_preds = np.average(solver.val_preds, weights=ckpt_weights, axis=0)\n",
    "# val_rho = compute_rho(y_val, val_preds[val_original_indices])\n",
    "# print('{\"metric\": \"Ckpt Val. Rho\", \"value\": %.4f}' % (val_rho,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
